<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Arnav Jain, Nilesh Gupta, Aditya Kusupati, Jon Barron, Deepak Pathak and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f5b461;
  text-decoration:none;
  }
  body,td,th {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 16px;
     font-weight: 400
  }
  heading {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 19px;
     font-weight: 600
  }
  strong {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 16px;
     font-weight: 800
  }
  sectionheading {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 22px;
     font-weight: 600
  }
  pageheading {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 48px;
     font-weight: 400
  }
  </style>

  <title>Vansh Ramani - Blog Posts & Tutorials</title>
  <link rel = "icon" href = "../images/IITD.png" type = "image/x-icon">
  <meta name="Vansh Ramani Blog" http-equiv="Content-Type" content="Vansh Ramani Blog">
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=PT+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="../css/darkmode.css">
  <script src="../js/darkmode.js"></script>
</head>

<body>
  <!-- Dark mode toggle button -->
  <button class="theme-toggle" onclick="toggleTheme()" title="Toggle dark mode">
    <i class="fa fa-moon-o" id="theme-icon"></i>
  </button>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
<tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	<p align="center">
		<pageheading>Blog Posts & Tutorials</pageheading><br>
		<a href="../index.html">← Back to Home</a>
	</p>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
	<tr><td> <sectionheading>&nbsp;&nbsp;Conference Experiences</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
		<td width="100%" valign="top">
			<p>
        <a href="#iclr2025" id="iclr2025">
          <heading>My ICLR 2025 Experience: From Computation Trees to Singapore's Skyline</heading></a><br>
        <em>Published: April 2025</em><br><br>
        
        Attending ICLR 2025 in Singapore felt like stepping into the beating heart of the machine learning research community. 
        As a second-year undergraduate from IIT Delhi, walking through the Singapore EXPO from April 24-28 and seeing my poster 
        displayed among the world's best research was nothing short of surreal.
        
        <br><br>
        <strong>Our Research: Bonsai - Redefining Graph Condensation</strong><br>
        Our paper, "Bonsai: Gradient-Free Graph Condensation for Node Classification," addressed a fundamental scalability challenge 
        in graph neural network training. Traditional graph condensation methods suffer from a critical flaw: they require training 
        a full GNN on the original dataset to extract gradients, defeating the very purpose of condensation.
        
        <br><br>
        Bonsai introduces a novel gradient-free methodology that constructs condensed graphs by selecting representative computation trees. 
        These trees capture how GNNs propagate information during message-passing operations. By leveraging the Weisfeiler-Lehman kernel 
        to measure structural similarity between computation trees, we developed a greedy selection algorithm that identifies exemplar trees.
        
        <br><br>
        The results were compelling. Across seven benchmark datasets including Reddit, PubMed, and OGB-ArXiv, Bonsai consistently 
        outperformed state-of-the-art baselines while being at least 7x faster in condensation time. The method requires no GPU acceleration, 
        consumes 17x less energy, and generalizes seamlessly across different GNN architectures like GCN, GAT, and GIN.
        
        <br><br>
        <strong>Special Encounters and Unexpected Connections</strong><br>
        One of the most memorable moments came when I noticed <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ" target="_blank">Prof. Michael Bronstein</a> 
        and his group positioned right across from our poster #186. After a brief conversation, he walked over to examine our work on Bonsai. 
        Standing there, explaining our gradient-free approach to one of the pioneers of geometric deep learning, felt like a defining moment of the conference.
        
        <br><br>
        The industry presence was overwhelming in the best possible way. Corporate booths from <a href="https://www.jumptrading.com/" target="_blank">Jump Trading</a>, 
        <a href="https://www.citadel.com/" target="_blank">Citadel</a>, <a href="https://www.janestreet.com/" target="_blank">Jane Street</a>, 
        <a href="https://www.google.com/" target="_blank">Google</a>, <a href="https://about.meta.com/" target="_blank">Meta</a>, and dozens of other companies 
        created a vibrant ecosystem where academic research met real-world applications.
        
        <br><br>
        <strong>Singapore: The Perfect Backdrop</strong><br>
        Between sessions, Singapore revealed its wonders. Gardens by the Bay provided a surreal backdrop for evening strolls, where conversations 
        about neural architecture search continued under the towering Supertrees. The marriage of technology and nature in that space felt 
        symbolic of the conference itself.
        
        <br><br>
        ICLR 2025 was more than a conference; it was a glimpse into the future of artificial intelligence research. As I returned to Delhi, 
        I carried with me not just memories of presentations and conversations, but a transformed understanding of what it means to be part 
        of the global AI research community.
        
        <br><br>
        <em>Authors: <a href="https://scholar.google.com/citations?user=mridul_id" target="_blank">Mridul Gupta</a>, 
        <a href="https://scholar.google.com/citations?user=samyak_id" target="_blank">Samyak Jain</a>, Vansh Ramani, 
        <a href="https://www.iitd.ac.in/~hariprasad/" target="_blank">Hariprasad Kodamana</a>, 
        <a href="https://www.cse.iitd.ac.in/~sayan/" target="_blank">Sayan Ranu</a></em><br>
        <em>Paper: Bonsai: Gradient-Free Graph Condensation for Node Classification</em><br>
        <em>Code: <a href="https://github.com/VanshRamani/bonsai" target="_blank">Available on GitHub</a></em>
			</p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
	<tr><td> <sectionheading>&nbsp;&nbsp;Technical Tutorials</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
		<td width="100%" valign="top">
			<p>
        <a href="#graph-distillation-tutorial" id="graph-distillation-tutorial">
          <heading>Understanding Graph Distillation: A Comprehensive Guide</heading></a><br>
        <em>Published: March 2025</em><br><br>
        
        Graph Neural Networks (GNNs) have revolutionized how we process relational data, but training on massive graphs 
        can be computationally prohibitive. Graph distillation emerges as a powerful solution to this challenge.
        
        <br><br>
        <strong>What is Graph Distillation?</strong><br>
        Graph distillation seeks to create a smaller, high-quality version of a graph that retains essential information—allowing 
        for faster, more efficient training without needing extensive computational resources. Think of it as creating a 
        "summary" of your graph that maintains the learning capacity of the original dataset.
        
        <br><br>
        <strong>The Problem with Existing Methods</strong><br>
        Most existing methods require a full training pass on the original graph dataset, which can undermine the goal 
        of efficient distillation. This is where our work on Bonsai comes in.
        
        <br><br>
        <strong>Introducing Bonsai: A Novel Approach</strong><br>
        Bonsai is a unique, linear-time, model-agnostic graph distillation algorithm that overcomes traditional limitations 
        by distilling graphs 22x faster than previous methods. It achieves state-of-the-art results on 14 out of 18 benchmark scenarios.
        
        <br><br>
        <strong>Key Technical Innovations:</strong><br>
        • <strong>Model-Agnostic Distillation:</strong> A single distilled dataset works across GNN architectures (GCN, GAT, GraphSage, GIN)<br>
        • <strong>CPU-Optimized:</strong> Designed to run efficiently without relying on expensive GPUs<br>
        • <strong>Linear-Time Performance:</strong> Bonsai mirrors the input space, bypassing full-dataset training
        
        <br><br>
        <strong>Implementation Details</strong><br>
        The core insight behind Bonsai lies in leveraging computation trees and the Weisfeiler-Lehman kernel to measure 
        structural similarity. Our greedy selection algorithm identifies exemplar trees with high representative power and diversity.
        
        <br><br>
        <em>For the complete technical implementation and code, visit our 
        <a href="https://github.com/VanshRamani/bonsai" target="_blank">GitHub repository</a>.</em>
			</p>
    </td>
  </tr>
  
  <tr>
		<td width="100%" valign="top">
			<p>
        <a href="#molecular-gnn-tutorial" id="molecular-gnn-tutorial">
          <heading>Graph Neural Networks for Molecular Property Prediction</heading></a><br>
        <em>Published: February 2025</em><br><br>
        
        Predicting molecular properties is crucial in drug discovery and materials science. Traditional methods rely on 
        expensive experimental data or quantum chemical calculations. Our approach using Graph Neural Networks offers 
        a more efficient alternative.
        
        <br><br>
        <strong>The MolMerger Approach</strong><br>
        We developed a novel "MolMerger" algorithm that creates virtual bonds between solute and solvent molecules, 
        allowing the neural network to learn from the structural nuances of molecular interactions without expensive 
        quantum calculations.
        
        <br><br>
        <strong>Key Results:</strong><br>
        • R² score of 0.94 for aqueous solubility prediction<br>
        • R² of 0.767 and MAE of 0.78 on test set<br>
        • Average MAE of 0.79 across 65 different solvents<br>
        • SHAP analysis revealing key molecular features
        
        <br><br>
        <strong>Technical Implementation</strong><br>
        Our framework combines Graph Neural Networks with attention mechanisms and GRUs. The key innovation lies in 
        how we represent solute-solvent pairs through virtual molecular bonds, enabling the model to capture complex 
        intermolecular interactions.
        
        <br><br>
        <em>Published in Journal of Chemical Theory and Computation, ACS. 
        <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.4c00382" target="_blank">Read the full paper</a> | 
        <a href="https://github.com/VanshRamani/Molmerger-Solubility-Prediction" target="_blank">GitHub Code</a></em>
			</p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
	<tr><td> <sectionheading>&nbsp;&nbsp;Research Insights</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
		<td width="100%" valign="top">
			<p>
        <a href="#high-dim-search" id="high-dim-search">
          <heading>Tackling the Curse of Dimensionality: Advanced Nearest Neighbor Search</heading></a><br>
        <em>Published: January 2025</em><br><br>
        
        During my internship at the <a href="https://www.ku.dk/english/" target="_blank">University of Copenhagen</a>, 
        I worked on developing Panorama, an exact k-nearest neighbor search algorithm that mitigates the curse of 
        dimensionality through innovative approaches.
        
        <br><br>
        <strong>The Challenge</strong><br>
        High-dimensional data complicates similarity search operations, making traditional kNN and RkNN computationally 
        expensive. This is particularly problematic in applications like location-based services, social networks, and 
        recommendation systems.
        
        <br><br>
        <strong>Our Solution: Panorama</strong><br>
        Panorama achieves O(n log d) query complexity by progressively pruning distance computations in high-dimensional 
        spaces (d ≈ 10⁶). The algorithm demonstrates a 10× speedup over ANNOY and HNSW baselines on image classification 
        and location-based service datasets.
        
        <br><br>
        <strong>Key Technical Innovations:</strong><br>
        • DCT energy compaction for dimension reduction<br>
        • Cauchy-Schwarz bounds for efficient distance computation<br>
        • Parallel architecture achieving k-fold speedup with k workers<br>
        • Particularly effective for transient data in ML distillation pipelines
        
        <br><br>
        <em>This work is currently under review at ICML 2025.</em>
			</p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="center"><font size="2">
        <a href="../index.html">← Back to Home</a> | 
        <a href="https://github.com/VanshRamani" target="_blank">GitHub</a> | 
        <a href="https://www.linkedin.com/in/ramanivansh/" target="_blank">LinkedIn</a>
        </font>
        </p>
      </td>
    </tr>
</table>

</td></tr>
</table>

</body>

</html>